{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM9ZDrh31K98m3whKKnTAEO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pratyushk2003/predis-prject/blob/main/Untitled33.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo==3.12.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfskD8VKlQKt",
        "outputId": "105b95eb-61d6-42ae-eff2-4367ba0cd3ad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymongo==3.12.0 in /usr/local/lib/python3.10/dist-packages (3.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "QSDWRweelDiN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MONGO_REMOTE_IP = \"44.196.167.97\"\n",
        "MONGO_REMOTE_PORT = 27017\n",
        "MONGO_AUTH_DB = \"admin\"\n",
        "MONGO_USER = \"dipsidpara@predis.ai\"\n",
        "MONGO_PASSWD = \"dipsi@predis\"\n",
        "MONGOCLIENT = MongoClient(host=MONGO_REMOTE_IP, port=MONGO_REMOTE_PORT, username=MONGO_USER, password=MONGO_PASSWD,\n",
        "                          authSource=MONGO_AUTH_DB)\n",
        "DB_INSTANCE = MONGOCLIENT[\"predictt_social\"]"
      ],
      "metadata": {
        "id": "4Y7gXuQulDfk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = list(DB_INSTANCE[\"content_calendar_configurations\"].find({\"config_type\": \"post_idea\"}).sort([(\"_id\", -1)]).limit(300000))\n",
        "outputs = list(DB_INSTANCE[\"content_calendar_ideas\"].find({}, {\"config_id\": 1, \"creative_details.modifications\": 1}).sort([(\"_id\", -1)]\n",
        "                                                                                                                          ).limit(300000))"
      ],
      "metadata": {
        "id": "QxQ8WYLTlDc1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_inputs = []\n",
        "config_ids = []\n",
        "for i in inputs:\n",
        "    try:\n",
        "        final_inputs.append({\n",
        "            \"post_idea\": i[\"post_settings\"][\"post_idea\"],\n",
        "            \"config_id\": str(i[\"_id\"]),\n",
        "            \"input_language\": i[\"post_settings\"][\"input_language\"],\n",
        "            \"output_language\": i[\"post_settings\"][\"output_language\"]\n",
        "        })\n",
        "        config_ids.append(str(i[\"_id\"]))\n",
        "    except KeyError:\n",
        "        pass"
      ],
      "metadata": {
        "id": "90IzBPVolDXZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(final_inputs), len(config_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJnfpcs4lDSy",
        "outputId": "95ec32c6-0057-470b-d0dd-96607b290456"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(299958, 299958)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_outputs = []\n",
        "for i in outputs:\n",
        "    if len(i.get(\"creative_details\", [])) > 0:\n",
        "        t_texts = i[\"creative_details\"][0][\"modifications\"].get(\"template_text\")\n",
        "        if t_texts:\n",
        "            if isinstance(t_texts, list):\n",
        "                tt = t_texts[0]\n",
        "            else:\n",
        "                tt = t_texts\n",
        "            if i.get(\"config_id\"):\n",
        "                final_outputs.append({\n",
        "                    \"config_id\": i[\"config_id\"],\n",
        "                    \"output_texts\": tt\n",
        "                })"
      ],
      "metadata": {
        "id": "2PZAphz0lDP8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"outputs.json\", \"w\") as f:\n",
        "    json.dump(final_outputs, f)\n",
        "\n",
        "with open(\"inputs.json\", \"w\") as f:\n",
        "    json.dump(final_inputs, f)"
      ],
      "metadata": {
        "id": "CbcEUyF0lDMx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eY4w9hpylDEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qrxy5FJvUSKG",
        "outputId": "358f5e60-4609-4af8-8ac8-c0566ba1bfaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                post_idea  \\\n",
            "0                          Selecting the Right Toothbrush   \n",
            "1                    Benefit of wearing Khadi dress\\n\\n\\n   \n",
            "2       bathroom renovations easy fast and better, fam...   \n",
            "3       bathroom renovations easy fast and better, fam...   \n",
            "4       bathroom renovations easy fast and better, fam...   \n",
            "...                                                   ...   \n",
            "243033  book now to our 9th annual tantra festival and...   \n",
            "243034  book now to our 9th annual tantra festival and...   \n",
            "243035  book now to our 9th annual tantra festival and...   \n",
            "243036  book now to our 9th annual tantra festival and...   \n",
            "243037  book now to our 9th annual tantra festival and...   \n",
            "\n",
            "                       config_id input_language output_language  \\\n",
            "0       6581428d745073b3fa153e89        english         english   \n",
            "1       65814282745073b3fa153e87        english         english   \n",
            "2       65814235900c19c134f57f8e        english         english   \n",
            "3       65814235900c19c134f57f8e        english         english   \n",
            "4       65814235900c19c134f57f8e        english         english   \n",
            "...                          ...            ...             ...   \n",
            "243033  64f9976dff059ca388945ed8        english         english   \n",
            "243034  64f9976dff059ca388945ed8        english         english   \n",
            "243035  64f9976dff059ca388945ed8        english         english   \n",
            "243036  64f9976dff059ca388945ed8        english         english   \n",
            "243037  64f9976dff059ca388945ed8        english         english   \n",
            "\n",
            "                                             output_texts  \n",
            "0       {'title': 'the ultimate guide to finding your ...  \n",
            "1       {'h1': 'discover 5 surprising benefits of wear...  \n",
            "2       {'h1': 'unveiling the secrets of the perfect b...  \n",
            "3       {'h1': 'revamp your bathroom with these budget...  \n",
            "4       {'title': 'transform your bathroom with ease a...  \n",
            "...                                                   ...  \n",
            "243033  {'title': 'embark on an enlightening journey',...  \n",
            "243034  {'title': 'embark on an enlightening journey',...  \n",
            "243035  {'title': 'embark on an enlightening journey',...  \n",
            "243036  {'title': 'embark on an enlightening journey',...  \n",
            "243037  {'title': 'embark on an enlightening journey',...  \n",
            "\n",
            "[243038 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "json_file_path1 = '/content/inputs.json'\n",
        "json_file_path2 = '/content/outputs.json'\n",
        "\n",
        "with open(json_file_path1, 'r') as f1, open(json_file_path2, 'r') as f2:\n",
        "    data1 = json.load(f1)\n",
        "    data2 = json.load(f2)\n",
        "\n",
        "df1 = pd.DataFrame(data1)\n",
        "df2 = pd.DataFrame(data2)\n",
        "\n",
        "merged_df = pd.merge(df1, df2, on='config_id')\n",
        "\n",
        "merged_df['output_texts'] = merged_df['output_texts'].apply(lambda x: ' '.join(str(x).split()))\n",
        "merged_df['output_texts'] = merged_df['output_texts'].str.lower()\n",
        "\n",
        "csv_file_path = '/content/merged_file.csv'\n",
        "merged_df.to_csv(csv_file_path, index=False)\n",
        "print(merged_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_file_path = '/content/merged_file.csv'\n",
        "merged_df = pd.read_csv(csv_file_path)\n",
        "\n",
        "output_texts_column = merged_df['output_texts']\n",
        "data = [eval(entry) for entry in output_texts_column]\n",
        "\n",
        "new_df = pd.DataFrame()\n",
        "new_df['output_texts'] = [\n",
        "    f\"{entry['h1']}. {entry['h2']}\" if 'h1' in entry else f\"{entry['title']}. {entry['subtitle']}\"\n",
        "    for entry in data\n",
        "]\n",
        "\n",
        "merged_df['output_texts'] = new_df['output_texts']\n",
        "\n",
        "merged_df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "print(merged_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g5wLy8Ukrzg",
        "outputId": "dcae2171-21e3-419f-ddbc-a2af525e34d3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                post_idea  \\\n",
            "0                          Selecting the Right Toothbrush   \n",
            "1                    Benefit of wearing Khadi dress\\n\\n\\n   \n",
            "2       bathroom renovations easy fast and better, fam...   \n",
            "3       bathroom renovations easy fast and better, fam...   \n",
            "4       bathroom renovations easy fast and better, fam...   \n",
            "...                                                   ...   \n",
            "243033  book now to our 9th annual tantra festival and...   \n",
            "243034  book now to our 9th annual tantra festival and...   \n",
            "243035  book now to our 9th annual tantra festival and...   \n",
            "243036  book now to our 9th annual tantra festival and...   \n",
            "243037  book now to our 9th annual tantra festival and...   \n",
            "\n",
            "                       config_id input_language output_language  \\\n",
            "0       6581428d745073b3fa153e89        english         english   \n",
            "1       65814282745073b3fa153e87        english         english   \n",
            "2       65814235900c19c134f57f8e        english         english   \n",
            "3       65814235900c19c134f57f8e        english         english   \n",
            "4       65814235900c19c134f57f8e        english         english   \n",
            "...                          ...            ...             ...   \n",
            "243033  64f9976dff059ca388945ed8        english         english   \n",
            "243034  64f9976dff059ca388945ed8        english         english   \n",
            "243035  64f9976dff059ca388945ed8        english         english   \n",
            "243036  64f9976dff059ca388945ed8        english         english   \n",
            "243037  64f9976dff059ca388945ed8        english         english   \n",
            "\n",
            "                                             output_texts  \n",
            "0       the ultimate guide to finding your perfect too...  \n",
            "1       discover 5 surprising benefits of wearing khad...  \n",
            "2       unveiling the secrets of the perfect bathroom ...  \n",
            "3       revamp your bathroom with these budget-friendl...  \n",
            "4       transform your bathroom with ease and efficien...  \n",
            "...                                                   ...  \n",
            "243033  embark on an enlightening journey. elevating y...  \n",
            "243034  embark on an enlightening journey. elevating y...  \n",
            "243035  embark on an enlightening journey. elevating y...  \n",
            "243036  embark on an enlightening journey. elevating y...  \n",
            "243037  embark on an enlightening journey. elevating y...  \n",
            "\n",
            "[243038 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwSpSEBxhu2O",
        "outputId": "2c36b429-648f-4c39-96ec-064aa1a175c4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Check if a GPU is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "csv_file_path = '/content/merged_file.csv'\n",
        "merged_df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Convert column values to strings\n",
        "post_idea_column = merged_df['post_idea'].astype(str)\n",
        "output_texts_column = merged_df['output_texts'].astype(str)\n",
        "\n",
        "# Load a smaller SentenceTransformer model onto the GPU\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2').to(device)\n",
        "\n",
        "# Reduce batch size\n",
        "batch_size = 1000  # Adjust the batch size based on available memory\n",
        "\n",
        "# Initialize an empty DataFrame to store the results\n",
        "result_df = pd.DataFrame()\n",
        "\n",
        "# Encode and compute cosine similarity in batches\n",
        "for i in range(0, len(post_idea_column), batch_size):\n",
        "    post_idea_batch = post_idea_column.iloc[i:i + batch_size].tolist()\n",
        "    output_texts_batch = output_texts_column.iloc[i:i + batch_size].tolist()\n",
        "\n",
        "    post_idea_embeddings_batch = model.encode(post_idea_batch, convert_to_tensor=True)\n",
        "    output_texts_embeddings_batch = model.encode(output_texts_batch, convert_to_tensor=True)\n",
        "\n",
        "    # Move embeddings to CPU\n",
        "    post_idea_embeddings_batch = post_idea_embeddings_batch.to(\"cpu\")\n",
        "    output_texts_embeddings_batch = output_texts_embeddings_batch.to(\"cpu\")\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    cosine_scores_batch = util.cos_sim(post_idea_embeddings_batch, output_texts_embeddings_batch)\n",
        "\n",
        "    # Extract diagonal values\n",
        "    relevance_scores_batch = cosine_scores_batch.diagonal()\n",
        "\n",
        "    # Append to the result DataFrame\n",
        "    result_df_batch = pd.DataFrame({'relevance_scores': relevance_scores_batch.tolist()})\n",
        "    result_df = pd.concat([result_df, result_df_batch], ignore_index=True)\n",
        "\n",
        "# Combine the relevance scores with the original DataFrame\n",
        "merged_df = pd.concat([merged_df, result_df], axis=1)\n",
        "\n",
        "# Save to CSV\n",
        "merged_df.to_csv('/content/merged_file_with_relevance_scores.csv', index=False)\n",
        "\n",
        "print(merged_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhkcFds_5je0",
        "outputId": "4acc2241-333b-4e3c-8144-d5df4d01f266"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                post_idea  \\\n",
            "0                          Selecting the Right Toothbrush   \n",
            "1                    Benefit of wearing Khadi dress\\n\\n\\n   \n",
            "2       bathroom renovations easy fast and better, fam...   \n",
            "3       bathroom renovations easy fast and better, fam...   \n",
            "4       bathroom renovations easy fast and better, fam...   \n",
            "...                                                   ...   \n",
            "243033  book now to our 9th annual tantra festival and...   \n",
            "243034  book now to our 9th annual tantra festival and...   \n",
            "243035  book now to our 9th annual tantra festival and...   \n",
            "243036  book now to our 9th annual tantra festival and...   \n",
            "243037  book now to our 9th annual tantra festival and...   \n",
            "\n",
            "                       config_id input_language output_language  \\\n",
            "0       6581428d745073b3fa153e89        english         english   \n",
            "1       65814282745073b3fa153e87        english         english   \n",
            "2       65814235900c19c134f57f8e        english         english   \n",
            "3       65814235900c19c134f57f8e        english         english   \n",
            "4       65814235900c19c134f57f8e        english         english   \n",
            "...                          ...            ...             ...   \n",
            "243033  64f9976dff059ca388945ed8        english         english   \n",
            "243034  64f9976dff059ca388945ed8        english         english   \n",
            "243035  64f9976dff059ca388945ed8        english         english   \n",
            "243036  64f9976dff059ca388945ed8        english         english   \n",
            "243037  64f9976dff059ca388945ed8        english         english   \n",
            "\n",
            "                                             output_texts  relevance_scores  \n",
            "0       the ultimate guide to finding your perfect too...          0.735220  \n",
            "1       discover 5 surprising benefits of wearing khad...          0.801564  \n",
            "2       unveiling the secrets of the perfect bathroom ...          0.613517  \n",
            "3       revamp your bathroom with these budget-friendl...          0.660211  \n",
            "4       transform your bathroom with ease and efficien...          0.777060  \n",
            "...                                                   ...               ...  \n",
            "243033  embark on an enlightening journey. elevating y...          0.630056  \n",
            "243034  embark on an enlightening journey. elevating y...          0.630056  \n",
            "243035  embark on an enlightening journey. elevating y...          0.630056  \n",
            "243036  embark on an enlightening journey. elevating y...          0.630056  \n",
            "243037  embark on an enlightening journey. elevating y...          0.630056  \n",
            "\n",
            "[243038 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Check if a GPU is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "csv_file_path = '/content/merged_file.csv'\n",
        "merged_df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Convert column values to strings\n",
        "post_idea_column = merged_df['post_idea'].astype(str)\n",
        "output_texts_column = merged_df['output_texts'].astype(str)\n",
        "\n",
        "# Load a smaller SentenceTransformer model onto the GPU\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
        "\n",
        "# Reduce batch size\n",
        "batch_size = 1000  # You may need to adjust this based on available memory\n",
        "\n",
        "# Encode in batches\n",
        "post_idea_embeddings = []\n",
        "output_texts_embeddings = []\n",
        "\n",
        "for i in range(0, len(post_idea_column), batch_size):\n",
        "    post_idea_batch = post_idea_column.iloc[i:i + batch_size].tolist()\n",
        "    output_texts_batch = output_texts_column.iloc[i:i + batch_size].tolist()\n",
        "\n",
        "    post_idea_embeddings_batch = model.encode(post_idea_batch, convert_to_tensor=True)\n",
        "    output_texts_embeddings_batch = model.encode(output_texts_batch, convert_to_tensor=True)\n",
        "\n",
        "    post_idea_embeddings.append(post_idea_embeddings_batch)\n",
        "    output_texts_embeddings.append(output_texts_embeddings_batch)\n",
        "\n",
        "# Concatenate the batches\n",
        "post_idea_embeddings = torch.cat(post_idea_embeddings)\n",
        "output_texts_embeddings = torch.cat(output_texts_embeddings)\n",
        "\n",
        "cosine_scores = util.cos_sim(post_idea_embeddings, output_texts_embeddings)\n",
        "\n",
        "# Move the diagonal values to CPU for storing in DataFrame\n",
        "merged_df['relevance_scores'] = cosine_scores.diagonal().to(\"cpu\")\n",
        "\n",
        "merged_df.to_csv('/content/merged_file_with_relevance_scores.csv', index=False)\n",
        "\n",
        "print(merged_df)\n"
      ],
      "metadata": {
        "id": "mwjKXP6Q5jq-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EwJHQZQk9alC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jo0t94ve9aiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q8gpjjmh9afh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WuYNlIF69acm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Check if a GPU is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "csv_file_path = '/content/merged_file.csv'\n",
        "merged_df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Convert column values to strings\n",
        "post_idea_column = merged_df['post_idea'].astype(str)\n",
        "output_texts_column = merged_df['output_texts'].astype(str)\n",
        "\n",
        "# Load SentenceTransformer model onto the GPU\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
        "\n",
        "post_idea_embeddings = model.encode(post_idea_column.tolist(), convert_to_tensor=True)\n",
        "output_texts_embeddings = model.encode(output_texts_column.tolist(), convert_to_tensor=True)\n",
        "\n",
        "# Move the embeddings to CPU for cosine similarity computation\n",
        "post_idea_embeddings = post_idea_embeddings.to(\"cpu\")\n",
        "output_texts_embeddings = output_texts_embeddings.to(\"cpu\")\n",
        "\n",
        "cosine_scores = util.cos_sim(post_idea_embeddings, output_texts_embeddings)\n",
        "\n",
        "# Move the diagonal values to CPU for storing in DataFrame\n",
        "merged_df['relevance_scores'] = cosine_scores.diagonal().to(\"cpu\")\n",
        "\n",
        "merged_df.to_csv('/content/merged_file_with_relevance_scores.csv', index=False)\n",
        "\n",
        "print(merged_df)\n"
      ],
      "metadata": {
        "id": "Nd1bypXaqkU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Haj2bRTuqkSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AYchKicSqkP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "csv_file_path = '/content/merged_file.csv'\n",
        "merged_df = pd.read_csv(csv_file_path)\n",
        "\n",
        "post_idea_column = merged_df['post_idea']\n",
        "output_texts_column = merged_df['output_texts']\n",
        "\n",
        "post_idea_embeddings = model.encode(post_idea_column.tolist(), convert_to_tensor=True)\n",
        "output_texts_embeddings = model.encode(output_texts_column.tolist(), convert_to_tensor=True)\n",
        "\n",
        "cosine_scores = util.cos_sim(post_idea_embeddings, output_texts_embeddings)\n",
        "\n",
        "merged_df['relevance_scores'] = cosine_scores.diagonal().to(\"cpu\")\n",
        "\n",
        "merged_df.to_csv('/content/merged_file_with_relevance_scores.csv', index=False)\n",
        "\n",
        "print(merged_df)"
      ],
      "metadata": {
        "id": "VZwYgI3SlndD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "77f77b9e-af76-4ab5-c7c9-27ecb449af51"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-dc1196ca1496>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moutput_texts_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_texts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpost_idea_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost_idea_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0moutput_texts_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_texts_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Batches\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0msentences_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mTokenizes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \"\"\"\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_sentence_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mbatch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtext_tuple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0mbatch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0mbatch2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mto_tokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0AMY-pNyqkNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oIQ4qSwMlnaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lhUbFku5lnXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file_path = '/content/merged_file.csv'  # Replace with the actual path to your CSV file\n",
        "merged_df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Extract 'output_texts' column from the DataFrame\n",
        "output_texts_column = merged_df['output_texts']\n",
        "\n",
        "# Convert the 'output_texts' column to a list of dictionaries\n",
        "data = [eval(entry) for entry in output_texts_column]\n",
        "\n",
        "# Extract values and combine into a single sentence\n",
        "output_texts = []\n",
        "for entry in data:\n",
        "    if 'h1' in entry:\n",
        "        sentence = f\"{entry['h1']}. {entry['h2']}\"\n",
        "    else:\n",
        "        sentence = f\"{entry['title']}. {entry['subtitle']}\"\n",
        "    output_texts.append(sentence)\n",
        "\n",
        "# Convert the list to a NumPy array and then to a PyTorch tensor\n",
        "output_texts_array = np.array(output_texts, dtype=object)\n",
        "output_texts_tensor = torch.tensor(output_texts_array, dtype=torch.object)\n",
        "\n",
        "# Create a new DataFrame with the 'output_texts' column\n",
        "new_df = pd.DataFrame({'output_texts': output_texts_tensor})\n",
        "\n",
        "# Display the modified DataFrame\n",
        "print(new_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "U7cleIvQjN-O",
        "outputId": "ad716851-6c7d-4740-918a-dc7dc9fa77fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-17980f2ab919>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Convert the list to a NumPy array and then to a PyTorch tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0moutput_texts_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0moutput_texts_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_texts_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Create a new DataFrame with the 'output_texts' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module '{__name__}' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'object'"
          ]
        }
      ]
    }
  ]
}